---
title: "HW3"
author: "Anubhav Shankar"
date: "`r Sys.Date()`"
output: html_document
---

```{r Set the Working Directory, include=FALSE}
setwd("C:/Users/anubh/Desktop/Course Requisites/Fall 2022/MTH 522 - Advanced Mathematical Stats/Homeworks/HW3")
getwd()
```
<br/>
```{r Load required packages, echo=FALSE,include=FALSE}
library(tidyverse)
library(ggplot2)
library(pROC)
library(rpart)
library(rpart.plot)
library(caret)
library(pscl)
library(car)
library(GGally)
library(neuralnet)
library(titanic)
library(caTools)
library(AER)
library(arm)
library(Amelia)
library(modeest)
library(mice)
library(MASS)
library(cowplot)
library(randomForest)
library(e1071)
library(ROCR)
library(markdown)
```
<br/>
Let's generate a non-linear data-set on which Neural Networks outperforms Logistic Regression.

**Dataset Description**: The planar data is a **randomly generated non-linear dataset** which would need multiple decision boundaries to get an accurate classification or prediction.
<br/>
```{r Generate the dataset}
planar_dataset <- function(){ # A function makes it easier to modify the parameters, if required
set.seed(123) # To ensure reproducibility
m <- 800 # Total length of the data frame
N <- m/2 # Sample size
D <- 2 # Dimensions 
X <- matrix(0, nrow = m, ncol = D) # matrix where each row is a single example
Y <- matrix(0, nrow = m, ncol = 1) # label vector (0 for red, 1 for blue)
a <- 5 # Max value of the flower spread
for(j in 0:1){ # For loop to generate a random sample 
      ix <- seq((N*j)+1, N*(j+1)) # range for sequence generation
      t <- seq(j*3.12,(j+1)*3.12,length.out = N) + rnorm(N, sd = 0.2) # petal inclination 
      r <- a*sin(4*t) + rnorm(N, sd = 0.2) # radius
      X[ix,1] <- r*sin(t) # fills the first column of the X matrix
      X[ix,2] <- r*cos(t) # fills the second column of the X matrix
      Y[ix,] <- j # will have value between 0 & 1
}
d <- as.data.frame(cbind(X, Y)) # Create a data-frame
names(d) <- c('X1','X2','Y') # Rename the columns accordingly 
d
}
```
<br/>
```{r Planar Plot}
df <- planar_dataset()
df <- df[sample(nrow(df)), ] # Shuffle dataset
head(df)
ggplot(df, aes(x = X1, y = X2, color = factor(Y))) +
geom_point()
```
<br/>
```{r Split data into training and testing set}
set.seed(123) # For reproducibility
index <- sample(1:nrow(df), 0.8 * nrow(df), replace = FALSE) # Do a 80-20 train-test split
train <- df[index,]
test <- df[-index,]

glimpse(train) # Get a quick glance of the data-frame structure

glimpse(test)
```
<br/>
```{r Logistic Regression}
logit_train <- glm(Y ~ ., data = train, family = "binomial")
summary(logit_train) # Logistic regression using training set

logLik(logit_train) # Log likelihood of training set

with(logit_train, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)) # p-value of the training dataset

pR2(logit_train)["McFadden"] # Logistic Regression equivalent of R-squared

vif(logit_train) # Check for multicollinearity

predictions_test <- predict(logit_train, test, type = "response")
predict_binary_test <- ifelse(predictions_test > 0.5, 1, 0)
cftable_test <- table(predict_binary_test, test$Y)
cftable_test
accuracy_test <- sum(diag(cftable_test))/sum(cftable_test)
accuracy_test
sensitivity_test <- cftable_test[1]/(cftable_test[1] + cftable_test[2])
sensitivity_test
specificity_test <- cftable_test[4]/(cftable_test[3] + cftable_test[4])
specificity_test
ppv_test <- cftable_test[1]/(cftable_test[1] + cftable_test[3])
ppv_test
npv_test <- cftable_test[4]/(cftable_test[2] + cftable_test[4])
npv_test

error <- mean(predict_binary_test!=test$Y)
error
accuracy <- 1-error
accuracy

roc_test <- roc(test$Y,predictions_test)
plot.roc(roc_test, col=par("fg"),plot=TRUE,print.auc = FALSE, legacy.axes = TRUE, asp =NA)
roc_test

plot.roc(smooth(roc_test), col="blue", add = TRUE,plot=TRUE,print.auc = TRUE, legacy.axes = TRUE, asp =NA)
legend("bottomright", legend=c("Empirical", "Smoothed"),
       col=c(par("fg"), "blue"), lwd=2)
```
<br/>
From the above observations we can conclude that Logistic Regression (LR) is not an appropriate model for this data. This is primarily due to the fact that Logistic Regression by default creates only one decision boundary. However, the Planar dataset is intentionally in a non-linear space, owing to the utilization of Sine and Cosine functions, hence, LR will have a horrible performance under such circumstances as shown by the AUC plot above.
<br/>
```{r Neural Network}
seed <- 490
set.seed(seed)

planar_nn <- neuralnet(Y~., data = train, hidden = 5, rep = 100, stepmax = 10000) # Run a neural network

plot(planar_nn, rep = 1) #Plot the NN structure with a single repetition

predict <- compute(planar_nn, test[,c(1:2)]) # Predict the trained model on the test set 

#head(predict$net.result) # View the converged results - OPTIONAL!
p <- predict$net.result #Store the predicted values

prediction <- ifelse(p>0.5,1,0) # Create the decision boundaries
pred_table <- table(prediction,test$Y) # Generate a confusion matrix
pred_table
error_nn <- 1-sum(diag(pred_table))/sum(pred_table) # Error of the NN
error_nn
accuracy_nn <- 1-error_nn # Accuracy of the NN
accuracy_nn
```
<br/>
**From the above observations, it is clear that Neural Network (NN), accuracy ~94% vs. ~47%, vastly outperforms LR. This is primarily due to the non-linear structure of the data. Additionally, reduction or increase in the number of hidden layers has negligible impact on the resultant accuracy; however, it does increase the running time. Hence, to include more layers we can pair this with a few other techniques to see the improvement.**
<br/>

**Let's now see a dataset where Logistic Regression (LR) performs better than Neural Network (NN).**

**Datset Description**: The sinking of the Titanic is one of the most infamous shipwrecks in history.

On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.

While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.


In this competition, you’ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled "train.csv" and the other is titled "test.csv".

Train.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the “ground truth”.

The `test.csv` dataset contains similar information but does not disclose the “ground truth” for each passenger. It’s your job to predict these outcomes.

Using the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived.


<br/>
```{r Load the Data}
train <- titanic_train # The Training Set
test <- titanic_test # The Testing Set
glimpse(train) # Analyze the metadata
glimpse(test)


colSums(is.na(train) | train == " ") # Check for NAs and blank values
colSums(is.na(test) | test == " ")
```
<br/>
```{r Data Wrangling, echo=FALSE,include=FALSE}
train <- train %>% mutate(Source = "Train") # Introduce an identifier column
test <- test %>% mutate(Source = "Test")
names(train)
names(test)
test$Survived <- NA # Add the target variable in the Test set as it is not there by default
names(test)
full <- rbind(train,test) # Combine the train and test sets for a few feature engineering steps
names(full)
glimpse(full)


full$Title<-sapply(full$Name,function(x) strsplit(x,'[.,]')[[1]][2]) # Strip the title away from the name
full$Title<-gsub(' ','',full$Title) # Remove extra spaces
aggregate(Age~Title,full,median) # Just a random pivot to decide titles based on Age. Done in lines 58-60
full$Title[full$Title %in% c('Capt', 'Don', 'Major','Jonkheer')] <- 'Sir' 
full$Title[full$Title %in% c('Dona','the Countess' )] <- 'Lady'
full$Title[full$Title %in% c('Mlle','Mme','Ms')] <- 'Miss'

full$FamilySize<-full$Parch+full$SibSp+1 # Create a variable for Family Size. The +1 is because some children were with
# nannies whose Parch = 0
full$Mother<-0 # Create a mother identifier
full$Mother[full$Sex=='female' & full$Parch>0 & full$Age>18 & full$Title!='Miss']<-1
full$Child<-0 # Create a child identifier
full$Child[full$Parch>0 & full$Age<=12]<-1

Surname<-sapply(full$Name,function(x) strsplit(x,'[.,]')[[1]][1]) # Strip out the surname
FamilyId<-paste0(full$FamilySize,Surname) # Create a Family Identifier basis the size
full$FamilyId<-factor(FamilyId)
Family<-data.frame(table(FamilyId))
SmallFamily<-Family$FamilyId[Family$Freq<=2]
FamilyId[FamilyId %in% SmallFamily]<-'Small'
full$FamilyId2<-factor(FamilyId)

full$Deck<-sapply(full$Cabin, function(x) strsplit(x,NULL)[[1]][1]) #Ascertain the Deck occupied from Cabin details
full$Deck[is.na(full$Deck)]<-'N/A' # Fill the missing values

full$CabinNum<-sapply(full$Cabin,function(x) strsplit(x,'[A-Z]')[[1]][2]) #Ascertain the Cabin Number occupied from Cabin details
full$num<-as.numeric(full$CabinNum) # Fill the missing values
num<-full$num[!is.na(full$num)]
Pos<-kmeans(num,3)
full$CabinPos<-'N/A'
full$CabinPos[!is.na(full$num)]<-Pos$cluster
full$CabinPos<-factor(full$CabinPos)
full$num<-NULL

full$Embarked[is.na(full$Embarked)]<-'S' # Fill the blank Embarkation points with Southampton value

full$Fare[is.na(full$Fare)]<-median(full$Fare,na.rm=T) # Fill missing Fare values with the median fare

######### A novel technique to fill the missing Age values using the Decision Trees #####################

age_fit<-rpart(Age[!is.na(Age)]~Pclass+Title+Sex+SibSp+Parch+Fare,data=full[!is.na(full$Age),],method='anova')
full$Age[is.na(full$Age)]<-predict(age_fit,full[is.na(full$Age),])

full<-transform(full,
                Pclass=factor(Pclass),
                Sex=factor(Sex),
                Embarked=factor(Embarked),
                Title=factor(Title),
                Mother=factor(Mother),
                Child=factor(Child),
                FamilyId2=factor(FamilyId2),
                Deck=factor(Deck)
) # Change the required columns to factors

train<-full[full$Source=='Train',]
test<-full[full$Source=='Test',]
train$Survived<-factor(train$Survived) # Turn the target variable into a factor


train_copy <- train # Create a copy of the training set
test_copy <- test # Create a copy of the test set

names(train_copy)
train_copy <- train_copy[,-c(9,11,13:22)] # Drop some redundant columns which didn't yield any statistical significance in previous tries
names(train_copy)
colSums(is.na(train_copy))
names(test_copy)
test_copy <- test_copy[,-c(9,11,13:22)]
names(test_copy)
test_copy <- test_copy[,-c(1,4)]

# full_copy <- full
# names(full)
# names(full_copy)
# full_copy <- full_copy[,-c(9,11,13:22)]
# names(full_copy)
# full_copy <- full_copy[,-c(1,4)]

glimpse(train_copy)
sapply(train_copy, function(x) length(unique(x)))
train_copy <- train_copy[,-c(1,4)]
names(train_copy)

### As the testing set provided by Kaggle does not have the target variable,
### a psuedo-random target variable is generated for the purpose of the project

## The step in line 151, however, will introduce bias to the testing set

test_copy <- test_copy %>% mutate(Survived = case_when(Sex == "male" ~ 0,
                                                       Pclass != "1" ~ 0,
                                                       Sex == "female" ~ 1))
test_copy$Survived <- as.factor(test_copy$Survived) # Turn the target variable into a factor

# full_copy <- full_copy %>% mutate(Survived = case_when((Survived != "0" || Survived != "1")&Sex == "male" ~ 0,
#                                                        (Survived != "0" || Survived != "1") & Pclass != "1" ~ 0,
#                                                        (Survived != "0" || Survived != "1") & Sex == "female" ~ 1))

# full_copy$Survived <- as.factor(full_copy$Survived)
# sum(is.na(full_copy$Survived))


colSums(is.na(train_copy))

colSums(is.na(test_copy))
```
<br/>
```{r Logistic Regression 2}
set.seed(444)

titanic_glm <- glm(Survived ~ Sex, data = train_copy, family = 'binomial')
summary(titanic_glm)

# Test for accuracy
predict_sex_survived <- predict(titanic_glm,newdata = test_copy,type = 'response') 
# Since Survived can only be either 1 or 0, write if statement to round up of down the response
predict_sex_survived <- ifelse(predict_sex_survived>0.5,1,0)
error_1 <- mean(predict_sex_survived!=test_copy$Survived)
accuracy_1 <- 1-error_1
accuracy_1

prob_survival <- data.frame(prob_survival = titanic_glm$fitted.values,Survived = train_copy$Survived, Sex = train_copy$Sex)

ggplot(prob_survival, aes(fill = Survived, y = prob_survival, x = Sex)) + 
  geom_bar(position = "fill", stat = "identity") + 
  labs(x= "Survival Status", y = "Probability of Survival", title = "Survival chances by Gender") + 
  scale_y_continuous(labels = scales::percent_format()) + 
  scale_fill_discrete(labels = c("Died","Survived"))


logistic_complete <- glm(Survived~., data = train_copy, family = "binomial")
summary(logistic_complete)
logistic_stepwise <- logistic_complete %>% stepAIC(direction = "both", trace = FALSE)
summary(logistic_stepwise)
AIC(logistic_complete,logistic_stepwise)


predict_logistic_test <- predict(logistic_stepwise,test_copy,type = "response")
predict_logistic_1 <- ifelse(predict_logistic_test>0.5,1,0)
cf_table_test <- table(predict_logistic_1,test_copy$Survived)
cf_table_test
acc_test <- sum(diag(cf_table_test))/sum(cf_table_test)
acc_test
sens_test <- cf_table_test[1]/(cf_table_test[1] + cf_table_test[2])
sens_test
spec_test <- cf_table_test[4]/(cf_table_test[3] + cf_table_test[4])
spec_test
ppvtest <- cf_table_test[1]/(cf_table_test[1] + cf_table_test[3])
ppvtest
npvtest <- cf_table_test[4]/(cf_table_test[2] + cf_table_test[4])
npvtest
error_test <- mean(predict_logistic_1!=test_copy$Survived)
acc_test <- 1 - error_test
acc_test

roc_test <- roc(test_copy$Survived,predict_logistic_test)
plot.roc(roc_test, col=par("fg"),plot=TRUE,print.auc = FALSE, legacy.axes = TRUE, asp =NA)
roc_test
plot.roc(smooth(roc_test), col="blue", add = TRUE,plot=TRUE,print.auc = TRUE, legacy.axes = TRUE, asp =NA)
legend("bottomright", legend=c("Empirical", "Test"),
col=c(par("fg"), "blue"), lwd=2)
```
<br/>

From the above observations it is clear that Logistic Regression does a great job in doing the requisite classifications. We'll now look at the performance of the Neural Networks on the same dataset below.

<br/>
```{r Neural Network 2}

set.seed(777)

control <- trainControl(method = "repeatedcv", number = 10, repeats = 10)

logistic_bayesian <- train(Survived ~ FamilySize+CabinPos+Deck+Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child, data = train, method = "bayesglm", trControl = control, family = binomial(link = "logit"))

acc_bayesian <- paste0(round(max(logistic_bayesian$results$Accuracy),3)*100,'%')
cat('prediction accucary with bayesian logistic regression is ',acc_bayesian,'.\n')

grid <- expand.grid(size = 1, decay = 0.01)

neuralnet <- train(Survived ~ FamilySize+CabinPos+Deck+Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child, data = train, method = "nnet", trControl = control,maxit = 1000, trace = FALSE, tuneGrid = grid )

acc_nnet <- paste0(round(max(neuralnet$results$Accuracy),3)*100,'%')
cat('prediction accucary of neural net is ',acc_nnet,'.\n')
```
<br/>
With a simple Logistic Regression, we achieved an accuracy of ~77%. Though Neural Networks achieved an accuracy of ~83%, another form of Logistic Regression - Bayesian Logistic Regression managed to do the same. Hence, from the above observations it is clear that Logistic is better suited for this dataset than Neural Network.
